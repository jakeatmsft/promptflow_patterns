$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json
environment:
  python_requirements_txt: requirements.txt
inputs:
  chat_history:
    type: list
    is_chat_history: true
    default: []
  subject_context:
    type: string
    is_chat_input: true
    default: Azure OpenAI
  question:
    type: string
    is_chat_input: true
    default: why doesn't prompt caching work in my test of the o1-mini model ?
outputs:
  answer:
    type: string
    reference: ${finance_autogen_step.output}
    is_chat_output: true
nodes:
- name: finance_autogen_step
  type: python
  source:
    type: code
    path: autogen_step.py
  inputs:
    aoai_conn: Default_AzureOpenAI
    bing_connection: bing_connection
    deployment_name: gpt-4o
    question: ${inputs.question}
    subject_context: ${inputs.subject_context}
