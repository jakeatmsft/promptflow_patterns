# Doc Extract Flow
The "Doc Extract Flow" is designed to process a given question and chat history to generate high-quality, accurate extracts from the original text, ensuring the outputs are free of hallucinations. It utilizes a series of nodes, each with a specific role, to interact with a Large Language Model (LLM) provided by Azure OpenAI. The flow takes two inputs: a list representing the chat history and a string input for the question to be processed, with a default question provided for demonstration purposes.

The flow produces two outputs: an "answer" which is the response generated by the chat node, and "reference_lines" which are the lines retrieved from the document that are presumably relevant to the provided question and chat history. This ensures that the extracts are grounded in the text, enhancing the reliability of the information provided.

The nodes involved in this flow include:

chat: Utilizes an LLM to process the question and generate an answer. It is configured with various parameters such as deployment name, temperature, and max tokens to tailor the response generation process.
prompt: Generates prompts for the LLM to ensure the questions are presented in a manner conducive to generating accurate and relevant answers.
doc: Intended to retrieve specific lines from the document based on the LLM's output, although the configuration for this node is incomplete in the provided excerpt.
This flow is particularly useful for extracting specific information from large documents, providing a tool for users to get precise answers along with the context directly from the source material. The emphasis on extracting line numbers and providing extracts free of hallucinations highlights the flow's focus on accuracy and reliability.


This flow is particularly useful for extracting specific information from large documents, providing a tool for users to get precise answers along with the context directly from the source material. The emphasis on extracting line numbers and providing extracts free of hallucinations highlights the flow's focus on accuracy and reliability.

## Inputs

- **chat_history**: A list representing the chat history. This input is not directly used as chat input but is marked as chat history.
- **question**: A string input for the question to be processed. The default question provided is "what is the 2nd amendment?".

## Outputs

- **answer**: A string output that contains the answer generated by the chat node.
- **reference_lines**: A string output that contains the reference lines retrieved, presumably relevant to the provided question and chat history.

## Nodes

### chat

- **Type**: llm (Large Language Model)
- **Source**: The code for this node is located in `chat.jinja2`.
- **Inputs**:
    - **Deployment Name**: Configuration for the deployment name.
    - **Temperature**: Configuration for the temperature.
    - **Top P**: Configuration for top_p.
    - **Max Tokens**: Configuration for max_tokens.
    - **Response Format**: Configuration for the response format.
    - **Presence Penalty**: Configuration for the presence penalty.
    - **Frequency Penalty**: Configuration for the frequency penalty.
    - **Sys Prompt**: Configuration for sys_prompt.
    - **Question**: Configuration for the question.
    - **Document**: Configuration for the document.
- **Connection**: Default_AzureOpenAI
- **API**: chat
- **Use Variants**: false

### prompt

- **Type**: prompt
- **Source**: The code for this node is located in `prompt.jinja2`.
- **Inputs**: This node does not take any inputs.
- **Use Variants**: false

### doc

- **Type**: prompt
- **Note**: The configuration for this node is incomplete in the provided excerpt.

## Usage

To effectively use this flow for extracting specific sections from large documents, such as the US Bill of Rights, follow these steps:

1. **Copy the Desired Long Context File**: Ensure the document you wish to query is accessible to the flow. This could involve uploading the document to a location from which the flow can retrieve it or ensuring it's in a format that the flow can process.

2. **Ask Context-Specific Questions**: Formulate your questions to be as specific as possible to the information you're seeking. For example, if you're interested in sections related to personal property within the US Bill of Rights, you might ask, "Which amendments in the Bill of Rights pertain to personal property?"

3. **Run the Flow**: With your document prepared and your question formulated, initiate the flow. The flow will process your input, interact with the configured LLM, and utilize the nodes to extract the relevant sections from your document.

4. **Review Outputs**: The flow will provide outputs in the form of an "answer" and "reference_lines". The answer will contain the LLM's response to your question, while the reference lines will include the specific sections of the document that are relevant to your query.

By following these steps, you can leverage the flow to extract precise information from extensive documents, ensuring that the extracts are accurate and free from hallucinations, thus providing a reliable tool for information retrieval.